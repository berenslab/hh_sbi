{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ephys_helper not found. Required for HH simulation.\n",
      "ephys_helper not found. Required for HH simulation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sbi.inference.snle import SNLE_A\n",
    "from sbi.utils.torchutils import BoxUniform\n",
    "from fslm.utils import OptimisedPrior, includes_nan\n",
    "from fslm.experiment_helper import SimpleDB\n",
    "from fslm4expdata import CalibratedLikelihoodEstimator, BiasEstimator, CalibratedPrior\n",
    "import pickle\n",
    "import subprocess\n",
    "from fslm.metrics import sample_kl\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sbi.analysis.plot import pairplot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(x, eps=1, rng=0):\n",
    "    np.random.seed(rng)\n",
    "    x_clean = x[~np.any(np.isnan(x), axis=1)]\n",
    "    x_clean = x_clean[~np.any(np.isinf(x_clean), axis=1)]\n",
    "        \n",
    "    noise_stats=np.random.multivariate_normal(\n",
    "        np.zeros((x.shape[1])),\n",
    "        eps*np.diag(x_clean.std(axis=0)),\n",
    "        size=x.shape[0]\n",
    "    )\n",
    "    return x + noise_stats.astype(np.float32)\n",
    "\n",
    "def optimise_base_prior_samples(sample_shape, bias_log_prob, base_prior_samples, seed=0):\n",
    "    n_samples = torch.Size(sample_shape).numel()\n",
    "    n = 0\n",
    "    samples = []\n",
    "    if seed != None: torch.manual_seed(seed); np.random.seed(seed)\n",
    "    # rejection sampling | could be replaced by sbi's rejection_sample\n",
    "    # return rejection_sample(self, self.base_prior, num_samples)\n",
    "\n",
    "    N = len(base_prior_samples)\n",
    "    while n < n_samples:\n",
    "        base_sample_idxs = torch.randperm(N)\n",
    "        theta = base_prior_samples[base_sample_idxs]\n",
    "        \n",
    "        p_accept = torch.exp(bias_log_prob(theta)).view(-1)\n",
    "        accepted = p_accept > torch.rand_like(p_accept)\n",
    "        samples += base_sample_idxs[accepted][:n_samples-n].tolist()\n",
    "        samples = list(set(samples)) # ensures samples are unique\n",
    "        n = len(samples)\n",
    "    return torch.tensor(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    /home/jnsbck/Uni/PhD/projects/fslm4expdata/fslm/code/fslm/experiment_helper.py:231: UserWarning: Existing database opened in write mode.                     Risk to existing files. Consider opening it as read only.\n",
      "  warnings.warn(\"Existing database opened in write mode. \\\n",
      " [py.warnings]\n"
     ]
    }
   ],
   "source": [
    "db_2d3M = SimpleDB(\"../data/2d3M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prior\n",
    "model_param_names = np.array(['C', r'$R_{input}$', r'$\\tau$', r'$g_{Nat}$', r'$g_{Na}$', r'$g_{Kd}$', r'$g_{M}$',\n",
    "                         r'$g_{Kv31}$', r'$g_{L}$', r'$E_{leak}$', r'$\\tau_{max}$', 'VT', 'rate_to_SS_factor'])\n",
    "prior_min = [0.1,  20,  0.1,    0,        0,      0,      0,      0,      0, -130,    50,    -90,   0.1]\n",
    "prior_max = [15,   1000,   70,   250,     100,      30,    3,     250,     3,  -50,  4000,   -35,    3]\n",
    "base_prior = BoxUniform(low=torch.as_tensor(prior_min), high=torch.as_tensor(prior_max))\n",
    "\n",
    "db_2d3M.write(\"base_prior\", base_prior)\n",
    "\n",
    "# All 25 degree Celcius mouse motor cortex (M1) electrophysiological data, preprocessed\n",
    "M1_25degree = pickle.load(open('../data/M1_features.pickle', 'rb'))\n",
    "Xo = M1_25degree['X_o']\n",
    "\n",
    "# write observations to database\n",
    "X_o = {key: list(val.values()) for key, val in Xo.T.to_dict().items()}\n",
    "db_2d3M.write(\"X_o\", X_o)\n",
    "\n",
    "# import simulations for training of NaN optimised prior\n",
    "num_sims = 30_000\n",
    "theta = torch.from_numpy(np.load(\"../data/full_batch.npz\")['theta'])\n",
    "X = np.load(\"../data/full_batch.npz\")['stats']\n",
    "\n",
    "torch.manual_seed(0)\n",
    "rd_idxs = torch.randperm(len(theta))[:num_sims]\n",
    "x4opt = torch.from_numpy(X)[rd_idxs]\n",
    "theta4opt = theta[rd_idxs]\n",
    "\n",
    "# add noise to simulations and write to database\n",
    "x2d4opt = torch.from_numpy(perturb(X, rng=0))[rd_idxs]\n",
    "db_2d3M.write(\"theta4opt\", theta4opt)\n",
    "db_2d3M.write(\"x4opt2d\", x2d4opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train optimised prior on\n",
    "# 30k noised data incl. NaNs -> prior_opt\n",
    "nan_bias = BiasEstimator(input_dim = theta.shape[1], model = \"resnet\")\n",
    "nan_bias.z_score_inputs(theta[:10_000])\n",
    "nan_bias.train(theta4opt, ~includes_nan(x2d4opt))\n",
    "prior_opt = CalibratedPrior(base_prior, nan_bias)\n",
    "prior_opt.bias.summarywriter = None # fix to allow pickling\n",
    "db_2d3M.write(\"prior\", prior_opt)\n",
    "\n",
    "## prepare training 3M noisy data points for posterior (incl. BiasEstimator)\n",
    "train_idxs = optimise_base_prior_samples((3_000_000,), prior_opt.bias.log_prob, theta, seed=1)\n",
    "theta_train = theta[train_idxs]\n",
    "x2d_train = torch.from_numpy(perturb(X, rng=0))[train_idxs]\n",
    "\n",
    "db_2d3M.write(\"x_2dopt\", x2d_train)\n",
    "db_2d3M.write(\"theta_2dopt\", theta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train posterior and bias estimator on the data\n",
    "# for s in range(10):\n",
    "#     subprocess.run(f\"python3 train_calibration_estimator.py -r {s} -d ../data/2d3M -t 2dopt\", shell=True, check=True)\n",
    "#     subprocess.run(f\"python3 train_nle_inference.py -r {s} -d 2d3M -f ../data/ -t 2dopt\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine likelihood and bias estimator to create nan calibrated posterior\n",
    "trained_inference_obj = db_2d3M.query(\"inference_2d\")\n",
    "calibration_estimator = db_2d3M.query(\"calibration_2d\")\n",
    "\n",
    "likelihood_estimator = trained_inference_obj._neural_net\n",
    "cal_likelihood = CalibratedLikelihoodEstimator(likelihood_estimator, calibration_estimator)\n",
    "cal_nle_posterior = trained_inference_obj.build_posterior(density_estimator=cal_likelihood, sample_with=\"mcmc\")\n",
    "\n",
    "# input = prior.sample((1,))\n",
    "# x_o_test = torch.tensor([X_o[\"20180918_sample_1\"]])[:,:-4]\n",
    "# assert cal_likelihood.log_prob(x_o_test, input.view(1,1,-1))\n",
    "\n",
    "db_2d3M.write(\"posterior_2d3M\", cal_nle_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample posterior for all observations\n",
    "# s = 0\n",
    "# for i in range(len(X_o)):\n",
    "#     subprocess.run(f\"python3 sample_hh_nle.py -o {i} -n xo_sweep -f ../data/ -d 2d3M -w 1 -t 2dopt -r {s}\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load npe posterior and database with nle samples\n",
    "with open(\"../data/training_schedule_2d.pickle\", \"rb\") as f:\n",
    "    npe_posterior = pickle.load(f)\n",
    "nle_sample_db = SimpleDB(\"../data/2d3M_xo_sweep\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and rank kls for all observations\n",
    "\n",
    "lkls = {}\n",
    "rkls = {}\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for cell, xo in tqdm(X_o.items()):\n",
    "    npe_samples = npe_posterior.sample((1000,), xo[:-4], show_progress_bars=False)\n",
    "    nle_samples = nle_sample_db.query(f\"samples_nle_2dopt_mcmc_{0}_{cell}_all_dims\")\n",
    "    lkls[cell] = sample_kl(nle_samples, npe_samples)\n",
    "    rkls[cell] = sample_kl(npe_samples, nle_samples)\n",
    "jsds = {cell:0.5*rkls[cell]+0.5*lkls[cell] for cell in X_o}\n",
    "\n",
    "sorted_rkls = dict(sorted(rkls.items(), key=lambda item: item[1]))\n",
    "sorted_lkls = dict(sorted(lkls.items(), key=lambda item: item[1]))\n",
    "sorted_jsds = dict(sorted(jsds.items(), key=lambda item: item[1]))\n",
    "\n",
    "cell_idxs = {k:v for k,v in zip(list(X_o), range(len(X_o)))}\n",
    "sorted_cells = list(sorted_rkls)\n",
    "sorted_cell_idxs = [cell_idxs[n] for n in sorted_rkls]\n",
    "\n",
    "sorted_kls = pd.DataFrame(data=np.vstack([sorted_cell_idxs, sorted_cells, list(sorted_rkls.values())]).T, columns=[\"cell_idx\", \"cell\", \"kl\"])\n",
    "sorted_kls.to_csv(\"../data/2d3M_xo_sweep/sorted_kls.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(list(sorted_rkls)[:50]).difference(set(os.listdir(\"../data/fslm_top50\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run fslm for 50 with lowest kl between npe and nle posteriors\n",
    "# import os\n",
    "# my_env = os.environ.copy()\n",
    "# my_env[\"LD_LIBRARY_PATH\"] = \":/home/jnsbck/Applications/anaconda3/envs/hh_sbi/lib\"\n",
    "\n",
    "# run fslm for 50 observations\n",
    "# for cell in top50_cells:\n",
    "#     for seed in range(5):\n",
    "#     subprocess.run(f\"python3 hh_fslm_tree.py -r {seed} -n fslm_top50/20180608_sample_2 -w 1 -d 2d3M/ -p 2d3M/ -f ../data/ -o 20180608_sample_2 -t 2dopt_mcmc_0_full\", shell=True, check=True, env=my_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh_sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04099293a75145a7f9ba9092e3952e70fcf0fbe081152e3a0da0f51f40b41ad2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
